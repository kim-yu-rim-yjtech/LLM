{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "\n",
    "def load_datasets(test_file):\n",
    "    df = pd.read_csv(test_file, encoding = 'cp949')\n",
    "    \n",
    "    list_dataset = {\n",
    "        'err_sentence': list(df['제목'])\n",
    "    }\n",
    "    \n",
    "    dataset_dict = {\n",
    "        'test': datasets.Dataset.from_dict(list_dataset, split='test')\n",
    "    }\n",
    "    dataset = datasets.DatasetDict(dataset_dict)\n",
    "    return dataset\n",
    "\n",
    "def get_ngram(text, n_gram):\n",
    "    ngram_list = []\n",
    "    text_length = len(text)\n",
    "    for i in range(text_length - n_gram + 1):\n",
    "        ngram_list.append(text[i:i+n_gram])\n",
    "    return ngram_list\n",
    "\n",
    "def calc_f_05(prd_sentence, n_gram):\n",
    "    prd_word_list = get_ngram(prd_sentence, n_gram)\n",
    "    #cor_word_list = get_ngram(cor_sentence, n_gram)\n",
    "    \n",
    "    cnt = 0\n",
    "    for idx in range(len(prd_word_list)):\n",
    "        start_idx = 0\n",
    "        end_idx = idx + 2\n",
    "        if idx > 2:\n",
    "            start_idx = idx - 2\n",
    "        # if prd_word_list[idx] in cor_word_list[start_idx:end_idx]:\n",
    "        #     cnt += 1\n",
    "    \n",
    "    if not prd_word_list:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    precision = cnt / len(prd_word_list)\n",
    "    # recall = cnt / len(cor_word_list)\n",
    "    \n",
    "    # if not (0.25 * precision + recall):\n",
    "    #     return 0, 0, 0\n",
    "    \n",
    "    # f_05 = 1.25 * (precision * recall) / (0.25 * precision + recall)\n",
    "    \n",
    "    return precision\n",
    "\n",
    "def my_train(gpus='cpu', model_path=None, test_file=None, eval_length=None, save_path=None, pb=False):\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, use_auth_token=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, use_auth_token=True)\n",
    "    \n",
    "    # load dataset\n",
    "    dataset = load_datasets(test_file)\n",
    "    \n",
    "    # model to device\n",
    "    device = torch.device(gpus)\n",
    "    model.to(device)\n",
    "    \n",
    "    # inference data and calculate scores(precision, recall, f0.5)\n",
    "    id_list = []\n",
    "    err_sentence_list = []\n",
    "    cor_sentence_list = []\n",
    "    prd_sentence_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f_05_list = []\n",
    "    \n",
    "    ngram = 6\n",
    "    data_len = len(dataset['test'])\n",
    "    bar_length = 100\n",
    "    if eval_length:\n",
    "        data_len = min(eval_length, len(dataset['test']))\n",
    "    _per_calc = 0.0\n",
    "    \n",
    "    print('=' * bar_length)\n",
    "    for n in tqdm(range(data_len), disable=pb):\n",
    "        data_id = dataset['test'][n]['id']\n",
    "        id_list.append(data_id)\n",
    "        err_sentence = dataset['test'][n]['err_sentence']\n",
    "        err_sentence_list.append(err_sentence)\n",
    "        # cor_sentence = dataset['test'][n]['cor_sentence']\n",
    "        # cor_sentence_list.append(cor_sentence)\n",
    "        tokenized = tokenizer(err_sentence, return_tensors='pt')\n",
    "        input_ids = tokenized['input_ids']\n",
    "        input_ids = input_ids.to(device)\n",
    "        res = model.generate(\n",
    "            inputs=input_ids,\n",
    "            num_beams=20,\n",
    "            num_return_sequences=2,\n",
    "            temperature=2,\n",
    "            repetition_penalty=0.2,\n",
    "            length_penalty=0.2,\n",
    "            no_repeat_ngram_size=2,\n",
    "            max_length=input_ids.size()[1] + 5).cpu().tolist()[0]\n",
    "        prd_sentence = tokenizer.decode(res).replace('<pad>', '').replace('<s>', '').replace('</s>', '').strip()\n",
    "        _cnt = n + 1\n",
    "        _per_calc = round(_cnt / data_len, 4)\n",
    "        _now_time = datetime.now().__str__()\n",
    "        _blank = ' ' * 30\n",
    "        print(f'[{_now_time}] - [{_per_calc:6.1%} {_cnt:06,}/{data_len:06,}] - Evaluation Result (Data id : {data_id})')\n",
    "        print(f'{_blank} >       TEST : {err_sentence}')\n",
    "        print(f'{_blank} >    PREDICT : {prd_sentence}')\n",
    "        # print(f'{_blank} >      LABEL : {cor_sentence}')\n",
    "        \n",
    "        # calculate scores\n",
    "        precision, recall, f_05 = calc_f_05(prd_sentence, ngram)\n",
    "        precision_list.append(precision)\n",
    "        # recall_list.append(recall)\n",
    "        # f_05_list.append(f_05)\n",
    "        print(f'{_blank} >  PRECISION : {precision:6.3f}')\n",
    "        # print(f'{_blank} >     RECALL : {recall:6.3f}')\n",
    "        # print(f'{_blank} > F0.5 SCORE : {f_05:6.3f}')\n",
    "        print('=' * bar_length)\n",
    "        prd_sentence_list.append(prd_sentence)\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # results save\n",
    "    _now_time = datetime.now().__str__()\n",
    "    save_file_name = os.path.split(test_file)[-1].replace('.json', '') + '.csv'\n",
    "    save_file_path = os.path.join(save_path, save_file_name)\n",
    "    _df = pd.DataFrame({\n",
    "        'id': data_id,\n",
    "        'err_sentence': err_sentence_list,\n",
    "        # 'prd_sentence': prd_sentence_list,\n",
    "        'cor_sentence': cor_sentence_list,\n",
    "        'precision': precision_list,\n",
    "        'recall': recall_list,\n",
    "        'f_05': f_05_list\n",
    "    })\n",
    "    _df.to_csv(save_file_path, index=True)\n",
    "    print(f'[{_now_time}] - Save Result File(.csv) - {save_file_path}')\n",
    "    \n",
    "    print('=' * bar_length)\n",
    "    # calculate presision, recall, F0.5 score by ngram-6\n",
    "    mean_precision = sum(precision_list) / len(precision_list)\n",
    "    # mean_recall = sum(recall_list) / len(recall_list)\n",
    "    # mean_f_05 = sum(f_05_list) / len(f_05_list)\n",
    "    \n",
    "    print(f'       Evaluation Ngram : {ngram}')\n",
    "    print(f'      Average Precision : {mean_precision:6.3f}')\n",
    "    # print(f'         Average Recall : {mean_recall:6.3f}')\n",
    "    # print(f'     Average F0.5 score : {mean_f_05:6.3f}')\n",
    "    print('=' * bar_length)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # parse inputs args\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--gpu_no\", dest=\"gpu_no\", type=int, action=\"store\")\n",
    "    parser.add_argument(\"--model_path\", dest=\"model_path\", type=str, action=\"store\")\n",
    "    parser.add_argument(\"--test_file\", dest=\"test_file\", type=str, action=\"store\")\n",
    "    parser.add_argument(\"--eval_length\", dest=\"eval_length\", type=int, action=\"store\")\n",
    "    parser.add_argument(\"-pb\", dest=\"pb\", action=\"store_true\")\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    # make save path\n",
    "    save_path = './data/results'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # get device\n",
    "    gpu_no = 'cpu'\n",
    "    if args.gpu_no or args.gpu_no == 0:\n",
    "        gpu_no = f'cuda:{args.gpu_no}'\n",
    "    \n",
    "    # get pb value\n",
    "    if args.pb:\n",
    "        args.pb = False\n",
    "    else:\n",
    "        args.pb = True\n",
    "    \n",
    "    _now_time = datetime.now().__str__()\n",
    "    print(f'[{_now_time}] ========== Evaluation Start ==========')\n",
    "    \n",
    "    # call main method\n",
    "    print(f'DEVICE : {gpu_no}, MODEL PATH : {args.model_path}, FILE PATH : 오탈자, DATA LENGTH : {args.eval_length}, SAVE PATH : {save_path}')\n",
    "    my_train(gpu_no, model_path=args.model_path, test_file='/home/yjtech2/Desktop/yurim/LLM/DATA/냄새_악취_포함.csv', eval_length=args.eval_length, save_path=save_path, pb=args.pb)\n",
    "    \n",
    "    _now_time = datetime.now().__str__()\n",
    "    print(f'[{_now_time}] ========== Evaluation Finished ==========')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
